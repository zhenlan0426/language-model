{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Penn Tree Bank (PTB) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    with open('simple-examples/data/'+path,'r') as f:\n",
    "        txt = f.read().replace('\\n',' ').split()\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = read_txt('ptb.train.txt')\n",
    "valid = read_txt('ptb.valid.txt')\n",
    "test = read_txt('ptb.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = Counter(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = dict(zip(vocabulary.keys(),range(V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict(zip(range(V),vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all([id2word[word2id[v]] == v for v in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2ids = lambda words: [word2id[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainNum = words2ids(train)\n",
    "validNum = words2ids(valid)\n",
    "testNum = words2ids(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "words,counts = zip(*vocabulary.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = [vocabulary[v]*1.0/len(train) for v in vocabulary.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6457861131858875"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy\n",
    "-np.sum(p * np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init for bias. SPEED UP TRAINING A LOT!!\n",
    "b0 = np.log(p,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(txt,batch_size,seq_len,random):\n",
    "    N = len(txt)\n",
    "    batch_tot_len = N/batch_size\n",
    "    bounds = np.arange(batch_size) * batch_tot_len \n",
    "    starts = np.array([np.random.randint(i,min(N,i+batch_tot_len)) for i in bounds]) if random else bounds\n",
    "    while np.all(starts+seq_len<np.minimum(bounds+batch_tot_len,N)):\n",
    "        yield np.array([txt[i:i+seq_len] for i in starts],dtype=np.int32),\\\n",
    "                np.array([txt[i+1:i+seq_len+1] for i in starts],dtype=np.int32)\n",
    "        starts += seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Computation Graph in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "seq_len = 15\n",
    "cells_dim = 256\n",
    "n_layers = 2\n",
    "learning_rate = 1e-1\n",
    "grad_clip = 5.0\n",
    "epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [batch_size, seq_len], name='X')\n",
    "Y = tf.placeholder(tf.int32, [batch_size, seq_len], name='Y')\n",
    "keep_prob = tf.placeholder(tf.float32,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(\"embedding\", [V, cells_dim],initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_embed = tf.nn.embedding_lookup(embedding,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_list = tf.unstack(X_embed,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(cells_dim),keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.MultiRNNCell([cell]*n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_state = cell.zero_state(batch_size,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputs, state = tf.contrib.rnn.static_rnn(cell,X_list,init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs_flat = tf.stack(outputs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('prediction'):\n",
    "    W = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[cells_dim, V],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(\n",
    "        \"b\",\n",
    "        initializer=tf.constant(b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = tf.einsum('blc,cd->bld',outputs_flat,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,logits=yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#train_op = optimizer.apply_gradients([(tf.clip_by_value(g,-grad_clip,grad_clip),v) \n",
    "#                                      for g,v in optimizer.compute_gradients(loss)])\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars),grad_clip)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build graph for sampling\n",
    "# input_sample = tf.placeholder(tf.int32, [batch_size], name='input_sample')\n",
    "# state_sample = cell.zero_state(batch_size,tf.float32)\n",
    "input_sample = tf.placeholder(tf.int32, [None], name='input_sample')\n",
    "state_sample = tuple([tf.placeholder(tf.float32, [None]+i.shape.as_list()[1:])\\\n",
    "                 for i in cell.zero_state(batch_size,tf.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sample, state_out_sample = cell(tf.nn.embedding_lookup(embedding,input_sample),state_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sample = tf.nn.softmax(tf.matmul(pred_sample,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(state0,input0,step):\n",
    "    # logP gives log prob of generated sentence\n",
    "    # output is a matrix of generated sentence with shape [step,batch]\n",
    "    n = input0.shape[0]\n",
    "    def normalize(x):\n",
    "        # due to rounding error some of sum(p)==1.000001\n",
    "        return x/np.sum(x)*0.999999\n",
    "    output = [[id2word[input_] for input_ in input0]]\n",
    "    logP = np.zeros(n)\n",
    "    for i in range(step):\n",
    "        pred_np, state0 = sess.run([pred_sample, state_out_sample],\\\n",
    "                                        {input_sample:input0,state_sample:state0,keep_prob:1})\n",
    "        input0 = [np.argmax(np.random.multinomial(1,normalize(p))) for p in pred_np]\n",
    "        output.append([id2word[input_] for input_ in input0])\n",
    "        logP += np.log(pred_np[range(n),input0])\n",
    "    return np.array(output),logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply2tuple(fun,tuple_):\n",
    "    return tuple([fun(t) for t in tuple_])\n",
    "def topK(x,k):\n",
    "    x = x.flatten()\n",
    "    args = np.argpartition(x, -k)[-k:]\n",
    "    return args, x[args]\n",
    "def indexSwitch(args,d):\n",
    "    return [(a//d,a%d) for a in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beamSearch(state0,input0,step,beamSize):\n",
    "    outputs = [[id2word[input0[0]]]*beamSize]\n",
    "    pred_np, state0 = sess.run([pred_sample, state_out_sample],\\\n",
    "                                    {input_sample:input0,state_sample:state0,keep_prob:1})  \n",
    "    state0 = apply2tuple(lambda x:np.broadcast_to(x,[beamSize]+list(x.shape[1:])),state0)\n",
    "    input0,logP = topK(np.log(pred_np),beamSize)\n",
    "    logP = np.reshape(logP,[-1,1])\n",
    "    outputs.append([id2word[i] for i in input0])\n",
    "    d = pred_np.shape[1]\n",
    "    \n",
    "    for i in range(step-1):\n",
    "        pred_np, state0 = sess.run([pred_sample, state_out_sample],\\\n",
    "                                        {input_sample:input0,state_sample:state0,keep_prob:1})     \n",
    "        logP = logP + np.log(pred_np)\n",
    "        index_,logP = topK(logP,beamSize)\n",
    "        logP = np.reshape(logP,[-1,1])\n",
    "        state_index,input0 = zip(*indexSwitch(index_,d))\n",
    "        state0 = apply2tuple(lambda x:np.array([x[j] for j in state_index]),state0)\n",
    "        outputs.append([id2word[j] for j in input0])\n",
    "    \n",
    "    return outputs,logP/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#state0,input0,step,beamSize = apply2tuple(lambda x:x[17:18],init_state_np),Y_np[17:18,-1],30,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:6.64664176513, Val loss:6.66929807215\n",
      "Train loss:6.08294783987, Val loss:6.06627306775\n",
      "Train loss:5.62807354364, Val loss:5.6894725877\n",
      "Train loss:5.41026022953, Val loss:5.51483056892\n",
      "Train loss:5.24682989175, Val loss:5.39557820711\n",
      "Train loss:5.13030031173, Val loss:5.32074147208\n",
      "Train loss:5.0438444433, Val loss:5.27096486092\n",
      "Train loss:4.95597624351, Val loss:5.22424374686\n",
      "Train loss:4.89249222304, Val loss:5.19757680608\n",
      "Train loss:4.82950742026, Val loss:5.1690716662\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    init_state_np = [np.zeros(init_.shape.as_list()) for init_ in init_state]\n",
    "    loss_train = 0\n",
    "    loss_val = 0\n",
    "    for X_np,Y_np in batch_iterator(trainNum,batch_size,seq_len,True):\n",
    "        _,init_state_np = sess.run([train_op,state],\\\n",
    "                                          {X:X_np,Y:Y_np,init_state:init_state_np,keep_prob:0.5})\n",
    "\n",
    "    if i%100 == 0:\n",
    "        init_state_np = [np.zeros(init_.shape.as_list()) for init_ in init_state]\n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "        for j,(X_np,Y_np) in enumerate(batch_iterator(trainNum,batch_size,seq_len,False)):\n",
    "            _,loss_j,init_state_np = sess.run([train_op,loss,state],\\\n",
    "                                              {X:X_np,Y:Y_np,init_state:init_state_np,keep_prob:0.5})\n",
    "            loss_train += loss_j\n",
    "        loss_train /= j\n",
    "\n",
    "        init_state_np = [np.zeros(init_.shape.as_list()) for init_ in init_state]\n",
    "        for j,(X_np,Y_np) in enumerate(batch_iterator(validNum,batch_size,seq_len,False)):\n",
    "            loss_j,init_state_np = sess.run([loss,state],\\\n",
    "                                            {X:X_np,Y:Y_np,init_state:init_state_np,keep_prob:1})\n",
    "            loss_val += loss_j\n",
    "        loss_val /= j   \n",
    "\n",
    "        print \"Train loss:{}, Val loss:{}\".format(loss_train,loss_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result,logP = sample(init_state_np,Y_np[:,-1],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"out <unk> of the professionals and the <unk> and the trade bill the troubles returned to be reached in analysts said takeover problems is more likely to stay <unk> a spokesman noted that a financial labor department has n't been rated pushed any sudden focus on the ual mercantile exchange closed offering in contrast copper and that await a short of the current term issues traders said that investors can report whether the rates toll for the facility last week a trade group georgia-pacific said but general co. runs a write-off of $ N million gm closed friday third-quarter average yesterday\""
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sample_result[:,np.argmax(logP)].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  4.,  7.,  4.,  1.,  1.,  0.,  0.,  0.,  1.]),\n",
       " array([-5.93002686, -5.66519238, -5.40035791, -5.13552343, -4.87068895,\n",
       "        -4.60585448, -4.34102   , -4.07618552, -3.81135105, -3.54651657,\n",
       "        -3.28168209]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ7gBoJj/AhFzFPnXVkPKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdcf376d790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(logP/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam Search finds sentence with a lot of the UNK tokens. As indicated by the average log probability, beam search does finds sentence with higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs,logP= beamSearch(apply2tuple(lambda x:x[7:8],init_state_np),Y_np[7:8,-1],30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.71813667],\n",
       "       [-1.71448636],\n",
       "       [-1.71206152],\n",
       "       [-1.70764816],\n",
       "       [-1.69857132],\n",
       "       [-1.70550406],\n",
       "       [-1.7016685 ],\n",
       "       [-1.70478404],\n",
       "       [-1.69957519],\n",
       "       [-1.69856513]], dtype=float32)"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tenure', 'the', 'the', 'house', '<unk>', 'the', 'house', '<unk>',\n",
       "       '<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', '<unk>', '<unk>',\n",
       "       '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'and', '<unk>',\n",
       "       '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>',\n",
       "       '<unk>'], \n",
       "      dtype='|S10')"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(outputs)[:,np.argmax(logP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
